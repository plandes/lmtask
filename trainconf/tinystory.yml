#@meta {desc: "tiny story trainer config", date: "2025-02-03"}
#@meta {url: "https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing"}
#@meta {doc: "some parameters overwrite defaults to make it easy to port this config"}


## Defaults
#
default:
  data_dir: ${default:root_dir}/data/tinystory

lmtask_default:
  base_resource: lmtask_llama_tinystory_inference_resource


## Train
#
lmtask_trainer_default:
  trainer_name: 'lmtask_trainer_hf'
  source_model: 'meta-llama/Llama-3.1-8B'

lmtask_tinystory_train_source:
  class_name: zensols.lmtask.dataset.LoadedTaskDatasetFactory
  source: roneneldan/TinyStories
  load_args:
    split: 'train[:2500]'
  task: 'instance: lmtask_task_tinystory'

lmtask_tinystory_eval_source:
  class_name: zensols.lmtask.dataset.LoadedTaskDatasetFactory
  source: roneneldan/TinyStories
  load_args:
    split: 'validation[:250]'
  task: 'instance: lmtask_task_tinystory'

lmtask_trainer_hf:
  train_source: 'instance: lmtask_tinystory_train_source'
  eval_source: 'instance: lmtask_tinystory_eval_source'

lmtask_trainer_hf_training_arguments:
  learning_rate: 5e-6
  num_train_epochs: 1
  optim: 'paged_adamw_32bit'
  lr_scheduler_type: 'cosine'
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  logging_steps: 2

lmtask_trainer_hf_peft:
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head
    - embed_tokens

lmtask_trainer_hf_peft:
  class_name: peft.LoraConfig
  r: 128
  lora_alpha: 32
  lora_dropout: 0.05

lmtask_llama_tinystory_hf_bnb_4bit_config:
  class_name: transformers.BitsAndBytesConfig
  load_in_4bit: true
  # quantize to 4-bit normalized float
  bnb_4bit_quant_type: 'nf4'
  bnb_4bit_compute_dtype: 'bfloat16'
  # apply double quantization for constants
  bnb_4bit_use_double_quant: true

lmtask_llama_imdb_model_args:
  device_map: auto
  # uncomment to load weights as a 16-bit float
  #torch_dtype: float16
  # uncommen to add quantization
  #quantization_config: 'instance: lmtask_llama_imdb_hf_bnb_4bit_config'


## Inference
#
lmtask_llama_tinystory_inference_resource:
  class_name: zensols.lmtask.llama.LlamaGeneratorResource
  model_desc: LlaMA 3.1 8B Instruct trained on the tiny story corpus
  model_args: 'instance: lmtask_llama_tinystory_model_args'
  model_id: '${lmtask_trainer_default:source_model}'
  peft_model_id: '${lmtask_trainer_default:peft_output_dir}'

lmtask_tinystory_stash:
  class_name: zensols.db.sqlite.SqliteDbStash
  path: 'path: ${default:data_dir}/task/tinystory.sqlite3'

lmtask_tinystory_caching_generator:
  class_name: zensols.lmtask.generate.CachingGenerator
  _delegate: 'alias: lmtask_default:base_generator'
  _stash: 'instance: lmtask_tinystory_stash'

lmtask_task_tinystory:
  class_name: zensols.lmtask.generate.GenerateTask
  description: 'trained on the tiny story corpus'
  request_class: 'class: zensols.lmtask.task.TaskRequest'
  response_class: 'class: zensols.lmtask.task.TaskResponse'
  1.condition:
    if: ${lmtask_default:cache}
    then:
      generator: 'instance: lmtask_tinystory_caching_generator'
    else:
      generator: 'alias: lmtask_default:base_generator'
  resource: 'instance: lmtask_llama_base_resource'
