#@meta {desc: "Stanford IMDB movie sentiment", date: "2025-02-05"}
#@meta {url: "https://huggingface.co/docs/trl/en/sft_trainer"}


## Defaults
#
default:
  data_dir: ${default:root_dir}/data/imdb

lmtask_default:
  lmtask_instruct_resource: lmtask_imdb_resource


## Train
#
lmtask_trainer_default:
  trainer_name: 'lmtask_trainer_hf'
  source_model: 'meta-llama/Llama-3.1-8B-Instruct'

lmtask_trainer_hf:
  source: 'instance: lmtask_imdb_source'

lmtask_trainer_hf_training_arguments:
  num_train_epochs: 1

# a dataset factory instance used by the trainer (lmtask_trainer_hf)
lmtask_imdb_source:
  class_name: zensols.lmtask.dataset.LoadedTaskDatasetFactory
  # the dataset name (downloaded if not already); this can be a `pathlib.Path`,
  # Pandas dataframe or Zensols Stash
  source: stanfordnlp/imdb
  # use only the training split
  load_args:
    split: train
  # the task that consumes the data, which will format each datapoint
  # specifically for that task's model
  task: 'instance: lmtask_task_imdb'
  # preprocessing Python source code to add labels and subset the data (db.select)
  pre_process: |-
    ds = ds.map(lambda x: {'output': 'positive' if x['label'] == 1 else 'negative'})
    ds = ds.rename_column('text', 'instruction')
    ds = ds.shuffle(seed=0)
    # 7K takes 55m
    ds = ds.select(range(7_000))


## Inference
#
lmtask_imdb_resource:
  class_name: zensols.lmtask.llama.LlamaGeneratorResource
  model_desc: LlaMA 3.1 8B Instruct trained on the IMDB corpus
  model_id: '${lmtask_trainer_default:checkpoint_dir}/checkpoint-437'
  model_args: 'instance: lmtask_llama_model_args'

lmtask_imdb_stash:
  class_name: zensols.db.sqlite.SqliteDbStash
  path: 'path: ${default:data_dir}/task/imdb.sqlite3'

lmtask_imdb_caching_generator:
  class_name: zensols.lmtask.generate.CachingGenerator
  _delegate: 'alias: lmtask_default:lmtask_instruct_generator'
  _stash: 'instance: lmtask_imdb_stash'

lmtask_task_imdb:
  class_name: zensols.lmtask.instruct.InstructTask
  description: 'trained on the Stanford IMDB movie sentiment corpus'
  request_class: 'class: zensols.lmtask.instruct.InstructTaskRequest'
  response_class: 'class: zensols.lmtask.task.TaskResponse'
  1.condition:
    if: ${lmtask_default:cache}
    then:
      generator: 'instance: lmtask_imdb_caching_generator'
    else:
      generator: 'alias: lmtask_default:lmtask_instruct_generator'
  resource: 'instance: lmtask_llama_instruct_resource'
  role: 'You are a sentiment classifier.'
  train_template: |-
    Only output the sentiment.
    ### Review:{{ instruction }}
    ### Sentiment:```{{ output }}```
  inference_template: |-
    Only output the sentiment.
    ### Review:{{ request.instruction }}
    ### Sentiment:
