#@meta {desc: "Stanford IMDB movie sentiment", date: "2025-02-05"}
#@meta {url: "https://huggingface.co/docs/trl/en/sft_trainer"}


## Defaults
#
default:
  data_dir: ${default:root_dir}/data/imdb

lmtask_default:
  instruct_resource: lmtask_imdb_inference_resource


## Train
#
lmtask_trainer_default:
  trainer_name: 'lmtask_trainer_hf'
  source_model: 'meta-llama/Llama-3.1-8B-Instruct'

# a dataset factory instance used by the trainer (lmtask_trainer_hf)
lmtask_imdb_train_source:
  class_name: zensols.lmtask.dataset.LoadedTaskDatasetFactory
  # the dataset name (downloaded if not already); this can be a `pathlib.Path`,
  # Pandas dataframe or Zensols Stash
  source: stanfordnlp/imdb
  # use only the training split
  load_args:
    split: train
  # the task that consumes the data, which will format each datapoint
  # specifically for that task's model
  task: 'instance: lmtask_task_imdb'
  # preprocessing Python source code to add labels and subset the data (db.select)
  pre_process: |-
    ds = ds.map(lambda x: {'output': 'positive' if x['label'] == 1 else 'negative'})
    ds = ds.rename_column('text', 'instruction')
    ds = ds.shuffle(seed=0)
    # 7K takes 55m
    ds = ds.select(range(7_000))


lmtask_trainer_hf:
  train_source: 'instance: lmtask_imdb_train_source'

lmtask_trainer_hf_training_arguments:
  num_train_epochs: 1
  optim: 'paged_adamw_32bit'
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  logging_steps: 2

lmtask_trainer_hf_peft:
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head
    - embed_tokens

lmtask_trainer_hf_peft:
  class_name: peft.LoraConfig
  r: 128
  lora_alpha: 32
  lora_dropout: 0.05

lmtask_llama_imdb_model_args:
  device_map: auto
  # uncomment to load weights as a 16-bit float
  #torch_dtype: float16
  # uncommen to add quantization
  #quantization_config: 'instance: lmtask_quant_4bit_config'


## Inference
#
lmtask_imdb_inference_resource:
  class_name: zensols.lmtask.llama.LlamaGeneratorResource
  model_desc: LlaMA 3.1 8B Instruct trained on the IMDB corpus
  model_args: 'asdict: lmtask_llama_imdb_model_args'
  model_id: '${lmtask_trainer_default:source_model}'
  peft_model_id: '${lmtask_trainer_default:peft_output_dir}'

lmtask_imdb_stash:
  class_name: zensols.db.sqlite.SqliteDbStash
  path: 'path: ${default:data_dir}/task/imdb.sqlite3'

lmtask_imdb_caching_generator:
  class_name: zensols.lmtask.generate.CachingGenerator
  _delegate: 'alias: lmtask_default:instruct_generator'
  _stash: 'instance: lmtask_imdb_stash'

lmtask_task_imdb:
  class_name: zensols.lmtask.instruct.InstructTask
  description: 'trained on the Stanford IMDB movie sentiment corpus'
  request_class: 'class: zensols.lmtask.instruct.InstructTaskRequest'
  response_class: 'class: zensols.lmtask.task.TaskResponse'
  1.condition:
    if: ${lmtask_default:cache}
    then:
      generator: 'instance: lmtask_imdb_caching_generator'
    else:
      generator: 'alias: lmtask_default:instruct_generator'
  resource: 'instance: lmtask_llama_instruct_resource'
  role: 'You are a sentiment classifier.'
  train_template: |-
    Only output the sentiment as either ```positive``` or ```negative```.
    ### Review:{{ instruction }}
    ### Sentiment:```{{ output }}```
  inference_template: |-
    Only output the sentiment as either 'positive' or 'negative'.
    ### Review:{{ request.instruction }}
    ### Sentiment:
